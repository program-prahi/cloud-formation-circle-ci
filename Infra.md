# Datalake for s3

This repo will allow you completely create a data lake in S3.  It consists of a series of top level CloudFormation templates. The template `datalake.yaml` at the current level is the parent template to be run in CloudFormation.  The templates under the `./datalake/` path are components to make nested stacks. Those templates are called from the current level `datalake.yaml` templates and never directly.


## Datalake Infrastructure

Components: 

[Datalake Infrastructure Stack](./datalake.yaml)

  - [KMS Customer Managed Key](./datalake/kms.yaml)
  - [S3 Raw Data Bucket](./datalake/bucket.yaml)
  - [S3 Refined Data Bucket](./datalake/bucket.yaml)
  - [S3 Transient Data Bucket](./datalake/bucket.yaml)
  - [S3 Glue Temporary Bucket](./datalake/bucket.yaml)
  - [S3 Athena Query Bucket](./datalake/bucket.yaml)
  - [Kinesis Data Stream](./datalake/kinesis-datastream.yaml)
  - [Kinesis Delivery Stream](./datalake/kinesis-firehose.yaml)
    - Glue Database
        - Database in Glue is a Meta Store of Data Stored in S3
        - Database created for Zones are
          - Raw
          - Transient
          - Refined
    - Glue Table
        - Tables in Glue are a part of the Database that groups related data stored in S3 in a path
        - Tables created for the datalake are
          - Raw Table to store CSV files generated out of Full Load DMS Task
          - CDC Table to store Parquet Files streamed out of Kinesis Firehose Delivery
          - Transient Tables - Data Present in Transient Zone
          - Refined Tabless - Data Present in Refined Zone
    - Log Group and Log Stream
        - Log Groups are logical grouping of logs to a particular AWS resource that generates it
        - Log Streams generated by an AWS Service that publishes logs to Amazon Cloudwatch
    - Kinesis Delivery Stream Policy
        - Allows Glue, S3, Lambda, Logs, Kinesis and KMS.
    - Delivery Stream Role
        - Role used by the Kinesis Firehose Delivery Streams governed by the policy
    - Delivery Stream
        - The Stream that collects data from Kinesis Data Streams and pushes to S3
  - [Public VPC](./datalake/default-vpc.yaml)
    - Internet Gateway and attach it to VPC
    - Public Subnets (2)
    - Public Route Tables for Public Subnets (2)
    - Security Group for inbound mysql from DB.
  - [DMS Replication Instance](./datalake/dmsreplicationinstance.yaml)
    - Replication Subnet Group
        - Replication Instance that uses a subnet group to deploy in a VPC
    - Replication Instance
        - The Compute instance on which replication tasks run
  - [DMS SNS Topic](./datalake/dms-sns-topic.yaml)
    - SNS Topic
        - The Topic for publishing notification messages
    - SNS Topic Policy
        - Permissions to trigger lambda
  - [DB Secret](./datalake/secret.yaml)
    - Secret to store the credentials of database
  - [DMS Endpoint](./datalake/dms-endpoint.yaml)
    - Source Endpoint
        - Connects to the Source database
    - DMS Kinesis Policy
        - Allows DMS to put records to kinesis data stream
    - DMS Kinesis Role
        - Policy when DMS Uses Kinesis As a Target 
    - DMS Kinesis Target Endpoint
        - CDC are pushed to Kinesis data stream
    - DMS S3 Policy
        - Allow DMS to put records at Raw bucket
    - DMS S3 Role
        - DMS S3 Policy
    - DMS S3 Target Endpoint
        - Full Load are pushed to raw bucket
  - [DMS Tasks](./datalake/dms-tasks.yaml)
    - DMS CDC Task
        - DMS performs Change Data Capture (CDC) migration of source database
    - DMS Full Load Task
        - DMS Performs Full Load Migration of source database migration once
    - DMS Event Subscription
        - DMS publishes notifications to SNS for changes like configuration,event,failure
  - [Glue IAM Roles and Policies](./datalake/iam.yaml)
    - Refined Bucket Access Policy
      - Allow read/write/list to Refined bucket
      - Allow KMS encrypt/decrypt
    - Raw Bucket Access Policy
      - Allow read/write/list to Raw bucket
      - Allow KMS encrypt/decrypt
    - Transient Bucket Access Policy
      - Allow read/write/list to Transient bucket
      - Allow KMS encrypt/decrypt
    - Glue Temp Bucket Access Policy
      - Allow read/write/list to Glue Temp bucket
      - Allow KMS encrypt/decrypt
    - Athena Query Bucket Access Policy
      - Allow read/write/list to Athena Query bucket
      - Allow KMS encrypt/decrypt
    - KMS Policy
      - Allow KMS encrypt/decrypt
    - Cloudwatch Logging Policy
      - Allow Writing Logs to Log Bucket
      - Allow KMS encrypt/decrypt
    - Invoke Lambda Policy
      - Allow invoking lambda function for account
    - Refined Bucket Role
      - Refined Bucket Access Policy
    - Raw Bucket Role
      - Raw Bucket Access Policy
    - Glue Repo Bucket Role
      - Glue Repo Bucket Access Policy
    - Glue Temp Bucket Role
      - Glue Temp Bucket Access Policy
    - Glue Refined Data Crawler Role
      - Allow Glue Service Role
      - S3 Refined Data Bucket Policy
    - Glue Raw Data Crawler Role
      - Allow Glue Service Role
      - S3 Raw Data Bucket Policy
    - Glue Etl Job Role
      - Allow Glue Service Role
      - S3 Refined Bucket Policy
      - S3 Raw Bucket Policy
      - S3 Glue Repo Bucket Policy
      - S3 Glue Temp Buvket Policy
      - Cloudwatch Logging Policy
      - Invoke Lambda Policy
      - Allow Delete Object in all Raw Bucket
      - Allow Delete Object in all Refined Bucket
      - Allow Delete Object in all Glue Repo Bucket
      - Allow Delete Object in all Glue Temp Bucket
    - Cloudwatch Event Role
      - Allow Cloudwatch Events Role
      - Cloudwatch Logging Policy
      - Invoke Lambda Policy
  - [Crawler to Scan RAW data tables](./datalake/glue-crawler.yaml)
    - Creates a Glue Database
    - Creates a Glue Crawler
    - Creates a Glue Security Configuration for the crawler to enforce SSE with the KMS Key
  - [Crawler to Scan REFINED data tables](./datalake/glue-crawler.yaml)
    - Creates a Glue Database
    - Creates a Glue Crawler
    - Creates a Glue Security Configuration for the crawler to enforce SSE with the KMS Key
  - [Crawler to Scan Transient data tables](./datalake/glue-crawler.yaml)
    - Creates a Glue Database
    - Creates a Glue Crawler
    - Creates a Glue Security Configuration for the crawler to enforce SSE with the KMS Key
  - [Crawler to Scan Transient Updates data tables](./datalake/glue-crawler.yaml)
    - Creates a Glue Database
    - Creates a Glue Crawler
    - Creates a Glue Security Configuration for the crawler to enforce SSE with the KMS Key
  - [Crawler to Scan Refined Updates data tables](./datalake/glue-crawler.yaml)
    - Creates a Glue Database
    - Creates a Glue Crawler
    - Creates a Glue Security Configuration for the crawler to enforce SSE with the KMS Key
  - [Glue Jobs](./datalake/glue-job.yaml)
    - Plain ETL Jobs for 10 tables of the FIG database. (10)
    - CDC job to capture the inserts of the FIG database. (1)
    - CDC job to capture the updates of the FIG database. (1)
    - Plain ETL jobs to transform the data in transient zone to refined zone. (10)
    - ETL job to transform the CDC updates data in transient zone to refined zone. (3)
    - Workflow Failure job (1)
    - Refined Workflow Kickoff job. (1)
  - [Glue Workflows](./datalake/glue-workflow.yaml)
    - Processing raw to transient workflow. 
    - Processing CDC - raw to transient workflow. 
    - Processing Transient to refined workflow. 
  - [Glue Triggers](./datalake.yaml)
    - Raw to Transient workflow kickoff trigger.
    - Raw to Transient ETL jobs trigger.
    - Raw workflow failure trigger.
    - CDC workflow kickoff trigger.
    - CDC inserts and updates job trigger.
    - Transient crawler trigger.
    - Transient - CDC updates Crawler trigger.
    - CDC workflow failure trigger.
    - Refined workflow kickoff job trigger.
    - Refined ETL jobs trigger.
    - Refined updates crawler trigger.
    - Refined crawler trigger.
    - Refined workflow failure trigger.
  - [FailureSNS](./datalake/sns.yaml)
    - SNS for workflow failures.
  - [Lambda Function](./datalake/lambdaNotification.yaml)
    - Starts the DMS CDC task and Processing raw to transient workflow when Full load DMS task succeeded.
  - [Athena Workgroup](./datalake.yaml)




# Deploying to New Environment and Setup

1.	Create new AWS Account
    1. Log into new AWS Account and create a non-root user with AdministratorAccess 
    2. Log in with new user

2.	Clone Git Repo
    1.	<Link>
    2.	Note local location of repo. Ex: /home/user/s3-data-lake
3. Open the /home/user/parameters.json file
    1. Give required parameters information as specified in the table below:


  |Parameter Name|Description & Utilization|Allowed Values|Default Value|Required?|
  |:---:|:---:|:---:|:---:|:---:|
  |CompanyName|S3 bucket naming conventions for the company the stuff is all for.|<Any String>|s3|Optional|
  |CodaS3BucketName|S3 bucket where the Quick Start templates and scripts are installed.|<Any String>|s3-datalake-templates-dev-394780878318|Optional|
  |CodaS3KeyPrefix|S3 key prefix used to simulate a folder for your copy of Quick Start assets.|<Any String>|data-lake/|Optional|
  |ProjectName|Name of the project for tagging purposes.|<Any String>|datalake|Optional|
  |Owner|Name of the Owner of the resources for tagging purposes.|<Any String>|Jacob Puthuparambil|Optional|
  |Environment|On which environment the resources to be created.| dev/ qa/ staging/ production| |Required|
  |VpcCIDR|The IP range (CIDR notation) for this VPC| '((\d{1,3})\.){3}\d{1,3}/\d{1,2}' |10.192.0.0/16|Optional|
  |PublicSubnet1CIDR|The IP range (CIDR notation) for this Public Subnet 1| '((\d{1,3})\.){3}\d{1,3}/\d{1,2}' |10.192.50.0/24|Optional|
  |PublicSubnet2CIDR|The IP range (CIDR notation) for this Public Subnet 2| '((\d{1,3})\.){3}\d{1,3}/\d{1,2}' |10.192.40.0/24|Optional|
  |ReplicationInstanceClass|Instance type of Replication Instance.| dms.t2.micro\ dms.t2.small\ dms.t2.medium\ dms.t2.large\ dms.c4.large\ dms.c4.xlarge\ dms.c4.2xlarge\ dms.c4.4xlarge|dms.c4.2xlarge|Optional|
  |EndpointBucketFolder|Prefix of raw bucket where the full load data populated| |fig_full_load|Optional|
  |Username|Username of the Database| |awsdatalake|Optional|
  |Password|Password of the Database| | |Required|
  |Engine|Engine type of the Database| | mysql |Optional|
  |Host|Host address of the Database| | xx.xx.xx.xxx |Optional|
  |Port|Port of the Database| | 3306 |Optional|
  |DBName|Name of the Database| | db_name |Optional|
  |RawInputDatabaseName|Name of the Raw Zone Database| |s3_dev_db_name_db_raw|Optional|
  |TransientInputDatabaseName|Name of the Transient Zone Database| |s3_dev_db_name_db_transient|Optional|
  |RefinedInputDatabaseName|Name of the Refined Zone Database| |s3_dev_db_name_db_refined|Optional|
  |EmailID|Email ID for sns to notify the glue workflow failure.| | |Required|

      Note:
        * “Optional” parameter indicates that if the parameter value is not passed then the default value will be used.
4.  Create a S3 Bucket to store the CloudFormation templates. 
      Note:
        * S3 bucket should be in the same region where the Cloudformation stacks wants to be deployed. 
5.	Create a secret that stores the details of the Source Database. The secret should contain:

  |Key|Value|
  |:---:|:---:|
  |Username|awsdatalake|
  |Password||
  |Host|xx.xx.xx.xxx|
  |DBName|db_name|

6.  Create a new user with programmatic access for deploying the stacks using CircleCI. Note the Key values to store them as environment variables.
7.  Open the CircleCI dashboard and Link the `s3-datalake` project.  
8.  Goto the Project Settings> Environment Variables, Add the following Environment Varibles to the project

  |Key|Description|
  |:---:|:---:|
  |AWS_ACCESS_KEY_ID|Access Key value of the user you have created before.|
  |AWS_SECRET_ACCESS_KEY|Secret Key value of the user you have created before.|
  |AWS_DEFAULT_REGION|Region where the stacks to be deployed.|
  |AWS_SECRET_NAME|Name of the AWS Secret that has the details of Database|
  |AWS_SECRET_REGION|Region of the AWS Secret that has the details of Database|
  |STACK_NAME|Name of the Cloudformation Stack|
  |SYNC_BUCKET_NAME|Name of the bucket where the Cloudformation templates to be stored|

9.  For every push to the `master` branch, CircleCI will create/update the stack.